{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TubeMap Tutorial\n",
    "\n",
    "This [notebook](TubeMap.ipynb) is a tutorial to run the main processing script that generates annotated graphs from vascualture lightsheet data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The source for this notebook can be found here: \n",
    "<a class=\"reference download internal\" download=\"\" href=\"TubeMap.ipynb\">\n",
    "    <code class=\"xref download docutils literal notranslate\"><span class=\"pre\">TubeMap.ipynb</span></code></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__    = 'Christoph Kirst <christoph.kirst.ck@gmail.com>'\n",
    "__license__   = 'GPLv3 - GNU General Pulic License v3 (see LICENSE.txt)'\n",
    "__copyright__ = 'Copyright © 2020 by Christoph Kirst'\n",
    "__webpage__   = 'http://idisco.info'\n",
    "__download__  = 'http://www.github.com/ChristophKirst/ClearMap2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize ClearMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ClearMap path\n",
    "import sys\n",
    "sys.path.append('/home/ckirst/Programs/ClearMap2')\n",
    "\n",
    "#load ClearMap modules\n",
    "from ClearMap.Environment import *  #analysis:ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Note\n",
    "    \n",
    "As ClearMap compiles its code on demand, executing this for the first time may take 5-30min.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize workspace\n",
    "\n",
    "The following sets up the directories and filenames for a TubeMap project. \n",
    "\n",
    "The raw files and files generated during the analysis of a data set are managed via the [workspace](Api/ClearMap.IO.Workspace.rst)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace[TubeMap]{/home/ckirst/Programs/ClearMap2/ClearMap/Tests/Data/TubeMap_Example}\n",
      "              raw: Raw/20-54-41_acta2_555-podo_cd31l_647_UltraII[<Y,2> x <X,2>]_C00_UltraII Filter0001.ome.tif {2 files, ('Y', 'X'): (1, 4) -> (1, 5)}\n",
      " autofluorescence: Autofluorescence/19-44-05_auto_UltraII_C00_xyz-Table Z<Z,4>.ome.tif {934 files, ('Z',): (0,) -> (933,)}\n",
      "         stitched: no file\n",
      "           layout: no file\n",
      "       background: no file\n",
      "        resampled: no file\n",
      "resampled_to_auto: no file\n",
      "auto_to_reference: no file\n",
      "         arteries: Raw/20-54-41_acta2_555-podo_cd31l_647_UltraII[<Y,2> x <X,2>]_C00_UltraII Filter0000.ome.tif {2 files, ('Y', 'X'): (1, 4) -> (1, 5)}\n",
      "           binary: no file\n",
      "    binary_status: no file\n",
      "         skeleton: no file\n",
      "            graph: no file\n",
      "          density: no file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#directories and files\n",
    "directory = '/home/ckirst/Programs/ClearMap2/ClearMap/Tests/Data/TubeMap_Example'    \n",
    "\n",
    "expression_raw      = 'Raw/20-54-41_acta2_555-podo_cd31l_647_UltraII[<Y,2> x <X,2>]_C00_UltraII Filter0001.ome.npy'          \n",
    "expression_arteries = 'Raw/20-54-41_acta2_555-podo_cd31l_647_UltraII[<Y,2> x <X,2>]_C00_UltraII Filter0000.ome.npy'       \n",
    "expression_auto     = 'Autofluorescence/19-44-05_auto_UltraII_C00_xyz-Table Z<Z,4>.ome.tif'  \n",
    "\n",
    "resources_directory = settings.resources_path\n",
    "\n",
    "ws = wsp.Workspace('TubeMap', directory=directory);\n",
    "ws.update(raw=expression_raw, arteries=expression_arteries, autofluorescence=expression_auto)\n",
    "ws.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a list of existing files in the project. As the project is new, only the raw data files and the autofluorescence images exist. \n",
    "\n",
    "* `directory` refers to the folder where the project data are located, and where all results will be written.\n",
    "\n",
    "\n",
    "* `expression_raw` refers to the source files for the raw vasculature data. It can be just a singe file name if \n",
    "  the data is stored in a single file. If the raw data comes as a list of files, this can be an   \n",
    "  [TagExpression(Api/ClearMap.Utils.TagExpression.html) of the file names.\n",
    "\n",
    "  For example, if the files written by the microscope are `UltraII[01 x 01].ome.tif`, `UltraII[02 x 01].ome.tif`,   etc, where the `[01 x 01]` part refres to the rows and columns of the individual stacks we can collect all those   files together to a common source using a tag expression `UltraII[<Y,2> x <X,2>].ome.tif`. \n",
    "\n",
    "  A tag is a place holder for a number or a name and has the general form `<Name, Type, Width>` (see    \n",
    "  [TagExpression](Api/ClearMap.Utils.TagExpression.rst), and if Type is obmitted the tag is assumed to be digits. \n",
    "  Name is the name of the tag and Width the number of chars or digits.\n",
    "\n",
    "  Here we use the names 'X' and 'Y' to denote the axes along the stacks are taken.\n",
    "\n",
    "  <div class=\"alert alert-info\">\n",
    "  Note \n",
    "      \n",
    "  The expression extension is `.npy` and not `.tif` as we’ll be working with this format for subsequent steps.\n",
    "  </div>\n",
    "  \n",
    "\n",
    "* `expression_arteries` refers to the arteries channel acquisition. It is handled the same way as \n",
    "  `expression_raw`.\n",
    "  \n",
    "* `expression_auto` refers to the autofluorescence channel acquisition. It is handled the same way as \n",
    "  `expression_raw`.\n",
    "\n",
    "   Here, the generic expression is similar to the one of the mosaic acquisitions, but in this case, each file is a \n",
    "   different plane, and not a different tile position. Thus we used `<Z,4>` in the expression.\n",
    "   By default the z-axis is asusmed to be along the stack direction in this script.\n",
    "   \n",
    "   <div class=\"alert alert-info\">\n",
    "   Tip\n",
    "    \n",
    "   You don’t have to type in the complex filenames yourself! In Linux Ubuntu, you can click on the file whose path\n",
    "   you want to get in the file explorer and ‘copy’ (‘ctrl + c’), and then go back on the script and ‘paste’\n",
    "   (`ctrl + v`), and the full path of the file will be put in.\n",
    "   \n",
    "   </div>\n",
    "       \n",
    "\n",
    "<div class=\"alert alert-info\">  \n",
    "Note\n",
    "    \n",
    "All file specificatoins are with respect to the working directory, which is automatically added to all paths. Here, `Raw/` or `Autofluorescence/` refers to the acquisition folder that was created by the microscope.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now access all file infomration through the `Workspace`. For example to get the file name of the raw data you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ckirst/Programs/ClearMap2/ClearMap/Tests/Data/TubeMap_Example/Raw/20-54-41_acta2_555-podo_cd31l_647_UltraII[<Y,2> x <X,2>]_C00_UltraII Filter0001.ome.npy'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws.filename('raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the file list contributing to the raw data set you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ckirst/Programs/ClearMap2/ClearMap/Tests/Data/TubeMap_Example/Raw/20-54-41_acta2_555-podo_cd31l_647_UltraII[01 x 04]_C00_UltraII Filter0001.ome.npy',\n",
       " '/home/ckirst/Programs/ClearMap2/ClearMap/Tests/Data/TubeMap_Example/Raw/20-54-41_acta2_555-podo_cd31l_647_UltraII[01 x 05]_C00_UltraII Filter0001.ome.npy']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws.file_list('raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also request access to the data via [ClearMap's IO Source interface](Api/ClearMap.IO.Source.rst):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileList-Source(404, 1302, 3501, 1, 2)[uint16]<2>{/home/ckirst/Programs/ClearMap2/ClearMap/Tests/Dat...raII[<Y,2> x <X,2>]_C00_UltraII Filter0001.ome.npy}\n",
      "(404, 1302, 3501, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "s = ws.source('raw')\n",
    "print(s)\n",
    "print(s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize alignment \n",
    "\n",
    "First initialize the atlas reference files by roughly slicing them to the part of the brain under consideration. \n",
    "E.g. here our data set is half a brain and we slice the corresponding hemis-shpere from the refernece atlas.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Note\n",
    "\n",
    "It is important that the alignment template and annotation file match the field of view of your acquisition. By default, they cover the whole brain, but if you acquired a smaller region of the brain (for instance only one hemisphere), you need to prepare new versions of these files to match the coverage of the brain from your acquisition. Also, the orientation in space of these images in these files needs to match the orientation of the brain during your acquisition. The original file themselves are located in the 'ClearMap/Ressources/Atlas’ folder and cropped files will be stored there as well.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing: '/home/ckirst/Programs/ClearMap2/ClearMap/Resources/Atlas/ABA_25um_annotation__1_-2_3__slice_None_None_None__slice_None_None_None__slice_0_256_None__.tif'\n",
      "Preparing: '/home/ckirst/Programs/ClearMap2/ClearMap/Resources/Atlas/ABA_25um_reference__1_-2_3__slice_None_None_None__slice_None_None_None__slice_0_256_None__.tif'\n",
      "Preparing: '/home/ckirst/Programs/ClearMap2/ClearMap/Resources/Atlas/ABA_25um_distance_to_surface__1_-2_3__slice_None_None_None__slice_None_None_None__slice_0_256_None__.tif'\n"
     ]
    }
   ],
   "source": [
    "#init atals and reference files\n",
    "annotation_file, reference_file, distance_file=ano.prepare_annotation_files(\n",
    "    slicing=(slice(None),slice(None),slice(0,256)), orientation=(1,-2,3),\n",
    "    overwrite=False, verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we initialize the parameter files to be used for the alignmnet via elastix.\n",
    "We provide a template files in ClearMap that typically dont need to be modified unless you \n",
    "experience problems with the alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alignment parameter files    \n",
    "align_channels_affine_file   = io.join(resources_directory, 'Alignment/align_affine.txt')\n",
    "align_reference_affine_file  = io.join(resources_directory, 'Alignment/align_affine.txt')\n",
    "align_reference_bspline_file = io.join(resources_directory, 'Alignment/align_bspline.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tile conversion\n",
    "\n",
    "ClearMap makes use of accessing binary files in parallel. \n",
    "For optimal performance we recommend to convert the data to numpy files.\n",
    "\n",
    "This step should take about 3 minutes. At the end, you will have a duplicate of all the original tif files with the npy format. It is recommended to delete the original tif files from the local disk to free up space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 2 files to npy!\n",
      "Converting file 0/2 Tif-Source(404, 1302, 3501)[uint16]{/home/ckirst/Programs/ClearMap2/ClearMap/Tests/Dat...47_UltraII[01 x 04]_C00_UltraII Filter0001.ome.tif} -> /home/ckirst/Programs/ClearMap2/ClearMap/Tests/Data/TubeMap_Example/Raw/20-54-41_acta2_555-podo_cd31l_647_UltraII[01 x 04]_C00_UltraII Filter0001.ome.npy\n",
      "Converting file 1/2 Tif-Source(404, 1302, 3501)[uint16]{/home/ckirst/Programs/ClearMap2/ClearMap/Tests/Dat...47_UltraII[01 x 05]_C00_UltraII Filter0001.ome.tif} -> /home/ckirst/Programs/ClearMap2/ClearMap/Tests/Data/TubeMap_Example/Raw/20-54-41_acta2_555-podo_cd31l_647_UltraII[01 x 05]_C00_UltraII Filter0001.ome.npy\n",
      "Converting 2 files to npy: elapsed time: 0:00:18.556\n"
     ]
    }
   ],
   "source": [
    "#%% Convet raw data to npy files     \n",
    "             \n",
    "io.convert_files(ws.file_list('raw', extension='tif'), extension='npy', \n",
    "                 processes=12, verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 2 files to npy!\n",
      "Converting file 0/2 Tif-Source(404, 1302, 3501)[uint16]{/home/ckirst/Programs/ClearMap2/ClearMap/Tests/Dat...47_UltraII[01 x 04]_C00_UltraII Filter0000.ome.tif} -> /home/ckirst/Programs/ClearMap2/ClearMap/Tests/Data/TubeMap_Example/Raw/20-54-41_acta2_555-podo_cd31l_647_UltraII[01 x 04]_C00_UltraII Filter0000.ome.npy\n",
      "Converting file 1/2 Tif-Source(404, 1302, 3501)[uint16]{/home/ckirst/Programs/ClearMap2/ClearMap/Tests/Dat...47_UltraII[01 x 05]_C00_UltraII Filter0000.ome.tif} -> /home/ckirst/Programs/ClearMap2/ClearMap/Tests/Data/TubeMap_Example/Raw/20-54-41_acta2_555-podo_cd31l_647_UltraII[01 x 05]_C00_UltraII Filter0000.ome.npy\n",
      "Converting 2 files to npy: elapsed time: 0:00:18.168\n"
     ]
    }
   ],
   "source": [
    "#%% Convert artery data to npy files      \n",
    "           \n",
    "io.convert_files(ws.file_list('arteries', extension='tif'), extension='npy', \n",
    "                 processes=12, verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "*ClearMap* comes with a many tools to visualize the data ([Visualization](Api/ClearMap.Visualization.rst))\n",
    "\n",
    "To visualize 3d image data ClearMap provides a data explorer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ClearMap.Visualization.Qt.DataViewer.DataViewer at 0x7fa6f7947690>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = ws.file_list('raw')[0]\n",
    "p3d.plot(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ClearMap's* data viewer opens in a new window alowing you to inspect the data:\n",
    "\n",
    "![DataViewer](Static/DataViewer.jpg)\n",
    "\n",
    "\n",
    "You can also open more files at the same time with synchronized windows to inspect them simultaneously. \n",
    "For this, pass a list of files to the plot command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ClearMap.Visualization.Qt.DataViewer.DataViewer at 0x7fe254318e10>,\n",
       " <ClearMap.Visualization.Qt.DataViewer.DataViewer at 0x7fdecbb377d0>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3d.plot(ws.file_list('raw')[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also overlay several files by specfying a inner list of files to overlay in a single window by passing a list  in the list of windows to open. So to open one windwo and overlap the files you can pass `[[source1, source2]]`, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ClearMap.Visualization.Qt.DataViewer.DataViewer at 0x7fa61833cc30>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3d.plot([ws.file_list('raw')[0:2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DataViewer Overlay](Static/DataViewer_overlay.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stitching\n",
    "\n",
    "Stitching is done using *ClearMap's* [WobblyStitcher](wobblystitcher.rst)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rigid z-alignment\n",
    "\n",
    "First all pairs of tiles are aligned along the z-axis.\n",
    "\n",
    "The parameters to adjust are:\n",
    "\n",
    "* `overlaps` is a tuple `(x-overlap, y-opverlap)` of the expected overlaps along the x- and y-axis d of the stacks\n",
    "  in pixels. \n",
    "  \n",
    "  E.g. if you set a 10% overlap on the microscope and the tile are (400,1300) in shape along x and y, you would \n",
    "  use (40,130), \n",
    " \n",
    "  Also, because the microscopes will usually have a delay in their real positions, it is often “safe” to set this \n",
    "  overlap a bit larger than the theoretical value. In this example, we set is to (45,155) instead of (40,130),\n",
    "  because we noticed the stage had a systematic error of 5 pixels in x and 15 pixels in y. \n",
    "  However, this tweak is not necessary.\n",
    "  \n",
    "  \n",
    "* `depth` is  a tuple `(x-depth, y-depth, z-depth)` the depth of the projection used at the shared border between\n",
    "  two tiles. The correct axis for maximum projection is determined automatically from the tiles.\n",
    "  \n",
    "  Use a value that is in the order of the overlap and includes enough infomration to enable alignment but not too \n",
    "  large so the maximum projection includes too much information only present in one tile.\n",
    "  \n",
    "  \n",
    "* `max_shifts` is a list of tuples of the form `[(x-min,x-max),(y-min,y-max),(z-min,z-max)]` and sets the \n",
    "  range of movement freedom for the tiles from their starting position. The starting position is the one defined \n",
    "  previously via overlaps. The larger the value, the more memory is necessary to compute the displacement.\n",
    "  \n",
    "  On our system, ±30 is a safe value for x and y.\n",
    "  \n",
    "  \n",
    "* `background` is a tuple `(threshold, number of pixels)` that sets the value of the background pixels and the\n",
    "   minimum num er of foreground pixels required to align two stacks. If `number of pixels` is a `float` it is\n",
    "   interpreted as required fraction of pixels in the overlap region to be foreground.\n",
    "\n",
    "   You can check the background easily by visualizing a tile as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment: rigidly aligning sources in layout: WobblyLayout<<2s, 1a>>P[0, 0, 0](763, 1302, 3501)[uint16]|F|!\n",
      "Alignment: aligning (4, 1) with (5, 1), alignment pair 0/1 !\n",
      "Alignment: aligning (4, 1) with (5, 1), alignment pair 0/1 done, shift = (0, 1, 2), quality = -8.14e+05!\n",
      "Alignment: rigidly aligning sources in layout: WobblyLayout<<2s, 1a>>P[0, 0, 0](763, 1302, 3501)[uint16]|F|!: elapsed time: 0:00:10.849\n",
      "Placement: placing WobblyLayout<<2s, 1a>>P[0, 0, 0](763, 1302, 3501)[uint16]|F|!\n",
      "Placement: placing WobblyLayout<<2s, 1a>>P[0, 0, 0](763, 1303, 3503)[uint16]|F|: elapsed time: 0:00:00.029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ckirst/Programs/ClearMap2/ClearMap/Tests/Data/TubeMap_Example/layout_aligned_axis_debug.lyt'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout = stw.WobblyLayout(expression=ws.filename('raw'), tile_axes=['X','Y'], overlaps=(45, 155));  \n",
    "\n",
    "st.align_layout_rigid_mip(layout, depth=[55, 165, None], max_shifts=[(-30,30),(-30,30),(-20,20)],\n",
    "                          ranges = [None,None,None], background=(400, 100), clip=25000, \n",
    "                           processes=None, verbose=True)\n",
    "\n",
    "st.place_layout(layout, method='optimization', min_quality=-np.inf, lower_to_origin=True, verbose=True)\n",
    "\n",
    "st.save_layout(ws.filename('layout', postfix='aligned_axis'), layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wobbly alignment\n",
    "\n",
    "The key parameters for the wobbly alignment are:\n",
    "\n",
    "* `max_shifts`: the movement freedom as before.\n",
    "\n",
    "\n",
    "* `validate_slice`: a dict spcifinh the parameters to determine if the alignment was succesfull.\n",
    "  \n",
    "  This is the most important parameter. Tiles at the border of the sample are challenging to place, because a lot of planes in these positions are outside the sample. Therefore, empty black planes outside the sample can easily be misplaced, and as a domino effect, this misplacement ripples through all connected tiles. To prevent tiles at the border of the scan to affect others, we set a minimal threshold under which the plane is not aligned, nor placed. Using the visualization tool as indicated above, find the lowest intensity level of a central tile in   the brain, usually deep in the scan. Set this minimal intensity value in `valid_range = (200,20000)`. The other number (20000) is the highest value encountered. If the value is set too high, this could lead to dropped planes inside the brain, which would appear as black tiles in the middle of the mosaic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment: aligning 1 pairs of wobbly sources.\n",
      "Alignment: wobbly alignment (4, 1)->(5, 1) along axis 2\n",
      "Alignment: Wobbly alignment (4, 1)->(5, 1) along axis 2: slice 0 / 3499\n",
      "Alignment: Not enough foreground pixels 0 < 1500 in range (200, 20000)!\n",
      "...\n",
      "Alignment: Wobbly alignment (4, 1)->(5, 1) along axis 2: slice 3300 / 3499\n",
      "Alignment: Wobbly alignment (4, 1)->(5, 1) along axis 2: slice 3400 / 3499\n",
      "Alignment: Wobbly slice alignment (4, 1)->(5, 1) along axis 2 done: elapsed time: 0:03:49.809\n",
      "Alignment: Wobbly alignment (4, 1)->(5, 1) along axis 2 done: elapsed time: 0:03:50.249\n",
      "Alignment: aligning 1 pairs of wobbly sources: elapsed time: 0:03:51.146\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ckirst/Programs/ClearMap2/ClearMap/Tests/Data/TubeMap_Example/layout_aligned_debug.lyt'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#layout = st.load_layout(ws.filename('layout', postfix='aligned_axis'))\n",
    "\n",
    "stw.align_layout(layout, axis_range=(None, None, 3), max_shifts=[(-30,30),(-15,15),(0,0)], axis_mip=None,\n",
    "                 validate=dict(method='foreground', valid_range=(200, None), size=None),\n",
    "                 prepare =dict(method='normalization', clip=None, normalize=True),\n",
    "                 validate_slice=dict(method='foreground', valid_range=(200,20000), size=1500),\n",
    "                 prepare_slice =None,\n",
    "                 find_shifts=dict(method='tracing', cutoff=3*np.sqrt(2)),\n",
    "                 processes=None, verbose=True)\n",
    "\n",
    "st.save_layout(ws.filename('layout', postfix='aligned'), layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wobbly placement\n",
    "\n",
    "If the alignment parameters are choosed well, this step should run without further parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placement: placing positions in 3503 slices!\n",
      "Placement: placing positions in 3503 slices done!: elapsed time: 0:00:01.238\n",
      "Placement: optimizing wobbly positions!\n",
      "Placement: found 1 components to optimize!\n",
      "Placement: optimizing component 0/1 with 3156 clusters!\n",
      "Placement: done constructing constraints for component 0/1!\n",
      "Placement: component 0/1 optimized!\n",
      "Placement: placing wobbly layout done!: elapsed time: 0:00:22.715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ckirst/Programs/ClearMap2/ClearMap/Tests/Data/TubeMap_Example/layout_placed_debug.lyt'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout = st.load_layout(ws.filename('layout', postfix='aligned'));\n",
    "\n",
    "stw.place_layout(layout, min_quality = -np.inf, \n",
    "                 method = 'optimization', \n",
    "                 smooth = dict(method = 'window', window = 'bartlett', window_length = 100, binary = None), \n",
    "                 smooth_optimized = dict(method = 'window', window = 'bartlett', window_length = 20, binary = 10),                             \n",
    "                 fix_isolated = False, lower_to_origin = True,\n",
    "                 processes = None, verbose = True)\n",
    "\n",
    "st.save_layout(ws.filename('layout', postfix='placed'), layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wobbly stitching\n",
    "\n",
    "This step stitches the data together into a large binary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stitching: stitching wobbly layout.\n",
      "Stitching: stitching 3503 sliced layouts.\n",
      "Stitching: stitching wobbly slice 0/3503\n",
      "Stitching: stitching wobbly slice 1/3503\n",
      "Stitching: stitching wobbly slice 2/3503\n",
      "...\n",
      "Stitching: stitching wobbly slice 3500/3503\n",
      "Stitching: stitching wobbly slice 3502/3503\n",
      "Stitching: stitching wobbly slice 3501/3503\n",
      "Stitching: stitching wobbly layout done!: elapsed time: 0:00:47.447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ckirst/Programs/ClearMap2/ClearMap/Tests/Data/TubeMap_Example/stitched_debug.npy'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout = st.load_layout(ws.filename('layout', postfix='placed'));\n",
    "\n",
    "stw.stitch_layout(layout, sink = ws.filename('stitched'), \n",
    "                  method = 'interpolation', processes='!serial', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ClearMap.Visualization.Qt.DataViewer.DataViewer at 0x7fefdec36410>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3d.plot(ws.filename('stitched'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will look something like this:\n",
    "    \n",
    "![Stitched](Static/DataViewer_stitched.png)\n",
    "\n",
    "You can use the mouse and click left to pan the view, or you can click right and drag to zoom in and out. Inspect the stitching, move the plane slider to look at other regions. If you see black regions in the mosaic, it is likely that the minimal intensity set in `valid_range` is too high. If you see misalignment, you can increase the search radius of `max_shifts` (for both `st.align_layout_rigid_mip` and `stw.align_layout`). If the mosaic looks good, you can delete the original individual tiles from the local disk to free up space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same layout is used to stitch the artery channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stitching: stitching wobbly layout.\n",
      "Stitching: stitching 3503 sliced layouts.\n",
      "Stitching: stitching wobbly slice 0/3503\n",
      "Stitching: stitching wobbly slice 1/3503\n",
      "Stitching: stitching wobbly slice 2/3503\n",
      "...\n",
      "Stitching: stitching wobbly slice 3500/3503\n",
      "Stitching: stitching wobbly slice 3501/3503\n",
      "Stitching: stitching wobbly slice 3502/3503\n",
      "Stitching: stitching wobbly layout done!: elapsed time: 0:00:44.028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ckirst/Programs/ClearMap2/ClearMap/Tests/Data/TubeMap_Example/stitched_arteries_debug.npy'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout = st.load_layout(ws.filename('layout', postfix='placed'));\n",
    "\n",
    "layout.replace_source_location(expression_raw, expression_arteries, method='expression')\n",
    "\n",
    "stw.stitch_layout(layout, sink = ws.filename('stitched', postfix='arteries'), \n",
    "                  method = 'interpolation', processes='!serial', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ClearMap.Visualization.Qt.DataViewer.DataViewer at 0x7f553c1357d0>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3d.plot([[ws.filename('stitched'), ws.filename('stitched', postfix='arteries')]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ImageProcessing Stitched](Static/ImageProcessing_stitched.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling and atlas alignment \n",
    "\n",
    "Here we will align the scan with the reference atlas. As mentioned before, make sure the template\n",
    "and annotation files are in the same orientation as the brain scan, and that their field of view \n",
    "covers more or less the same regions.\n",
    "\n",
    "\n",
    "### Resampling - raw data\n",
    "\n",
    "In the resampling and alignment step, there are only a few parameters to check:\n",
    "\n",
    "* `source_resolution` is the resolution of the data as `(x-resolution, y-resolution, z-resolution)`. \n",
    "\n",
    "   \n",
    "* `sink-resolution` is the resolution of the Atlas. We use the 25µm version of the annotation.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process     0: Resampling: resampling axes (0, 1), slice (0,) / 3503\n",
      "Process     1: Resampling: resampling axes (0, 1), slice (1,) / 3503\n",
      "Process     2: Resampling: resampling axes (0, 1), slice (2,) / 3503\n",
      "...\n",
      "Process    45: Resampling: resampling axes (1, 2), slice (45,) / 50\n",
      "Process    48: Resampling: resampling axes (1, 2), slice (48,) / 50\n",
      "Process    49: Resampling: resampling axes (1, 2), slice (49,) / 50\n",
      "Resampling: elapsed time: 0:00:09.397\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tif-Source(50, 85, 225)[uint16]{/home/ckirst/Programs/ClearMap2/ClearMap/Tests/Data/TubeMap_Example/resampled_debug.tif}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resample_parameter = {\n",
    "    \"source_resolution\" : (1.625,1.625,1.6),\n",
    "    \"sink_resolution\"   : (25,25,25),\n",
    "    \"processes\" : None,\n",
    "    \"verbose\" : True,             \n",
    "    };\n",
    "\n",
    "res.resample(ws.filename('stitched'), sink=ws.filename('resampled'), **resample_parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling - autofluorescence\n",
    "\n",
    "* `source_resolution` refers to the autofluorescence scan. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  resample_parameter_auto = {\n",
    "    \"source_resolution\" : (5,5,6),\n",
    "    \"sink_resolution\"   : (25,25,25),\n",
    "    \"processes\" : None,\n",
    "    \"verbose\" : True,                \n",
    "    };    \n",
    "\n",
    "res.resample(ws.filename('autofluorescence'), \n",
    "             sink=ws.filename('resampled', postfix='autofluorescence'), \n",
    "             **resample_parameter_auto)\n",
    "\n",
    "#p3d.plot([ws.filename('resampled'), ws.filename('resampled', postfix='autofluorescence')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment  - resampled to autofluorescence\n",
    "\n",
    "This step interfaces to [elastix](https://elastix.lumc.nl/) to aligin the resampled raw image to the resampled autofluorescence image.\n",
    "\n",
    "As the autofluorescence image is often taken in a separate step with different microscope settings both images typically do not align. This steps corrects for this via a affine transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align the two channels\n",
    "align_channels_parameter = {            \n",
    "    #moving and reference images\n",
    "    \"moving_image\" : ws.filename('resampled', postfix='autofluorescence'),\n",
    "    \"fixed_image\"  : ws.filename('resampled'),\n",
    "    \n",
    "    #elastix parameter files for alignment\n",
    "    \"affine_parameter_file\"  : align_channels_affine_file,\n",
    "    \"bspline_parameter_file\" : None,\n",
    "    \n",
    "    #directory of the alig'/home/nicolas.renier/Documents/ClearMap_Ressources/Par0000affine.txt',nment result\n",
    "    \"result_directory\" :  ws.filename('elastix_resampled_to_auto')\n",
    "    }; \n",
    "\n",
    "elx.align(**align_channels_parameter);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment - autofluorescence to reference\n",
    "\n",
    "This step aligins the resampled autofluorescence image to the atlas reference via a non-linear transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align autofluorescence to reference\n",
    "align_reference_parameter = {            \n",
    "    #moving and reference images\n",
    "    \"moving_image\" : reference_file,\n",
    "    \"fixed_image\"  : ws.filename('resampled', postfix='autofluorescence'),\n",
    "    \n",
    "    #elastix parameter files for alignment\n",
    "    \"affine_parameter_file\"  :  align_reference_affine_file,\n",
    "    \"bspline_parameter_file\" :  align_reference_bspline_file,\n",
    "    #directory of the alignment result\n",
    "    \"result_directory\" :  ws.filename('elastix_auto_to_reference')\n",
    "    };\n",
    "\n",
    "elx.align(**align_reference_parameter);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alignment step should last for about 2 to 10 minutes and generate two files: \n",
    "`ws.filename('resampled')` and `ws.filename('resampled', postfix='autofluorescence'))`, as well as 2 folders: `elastix_resampled_to_auto` and `elastix_auto_to_reference`. \n",
    "\n",
    "You can check the quality of the alignment. The easiest is to use [Fiji (Image J)](https://imagej.net/Fiji). Open the following files: `ws.filename('resampled')` and `ws.filename('resampled', postfix='autofluorescence'))`. These are the original we need to align. \n",
    "\n",
    "Then go into the `elastix_resampled_to_auto` folder. This is the alignment of the resampled data to the resampled autofluorescence image. Open the `result.0.mhd` file. \n",
    "\n",
    "Then go to the folder `elastix_auto_to_reference`. This is the result of the alignment of the autofluorescence to the Atlas reference. Open the `result.1.mhd file`. \n",
    "\n",
    "Organize the files as follows: \n",
    "\n",
    "![Alignment check](Static/Alignment_check.png)\n",
    "\n",
    "You can find the contrast adjustment panel in `Image -> Adjust -> Brightness/Contrast`. The synchronize windows tool can be found in `Analyze -> Tools -> Synchronize windows`. Click on `Synchronize all`, and travel through the stacks. With the mouse pointer, and make sure each aligned stack are well in sync with each other: make sure the outline of the brain is aligned, as well as the internal structures. Only the aligned data (images organized vertically here) have to match. If the alignment is good, you are then ready for the image processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test data\n",
    "\n",
    "This optional step allows to create a smaller sub-image from the full image in order to test the image processing\n",
    "pipeline that follows.\n",
    "\n",
    "When starting with a new data set, we highly recommend using this step to speed up processing and \n",
    "adjust the pipeline.\n",
    "\n",
    "Skip this if you dont need to test the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ckirst/Programs/ClearMap2/ClearMap/Tests/Data/TubeMap_Example/debug_stitched.npy',\n",
       " '/home/ckirst/Programs/ClearMap2/ClearMap/Tests/Data/TubeMap_Example/debug_stitched_arteries.npy']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% Crop test data \n",
    "\n",
    "#select debug mode / test file name\n",
    "ws.debug = True; \n",
    "#ws.debug = 'test'\n",
    "\n",
    "#select sublice for testing the pipeline\n",
    "slicing = (slice(0,400),slice(0,600),slice(1000,1300));\n",
    "[ws.create_debug('stitched', slicing=slicing),\n",
    "ws.create_debug('stitched', postfix='arteries', slicing=slicing)]\n",
    "\n",
    "#p3d.plot(ws.filename('stitched'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like to create various test sets you can give each subset a name by setting `ws.debug = 'name_for_the_test_subset'` in the above code and run it again. \n",
    "\n",
    "You can switch between test sets by using `ws.debug = 'name_for_the_test_subset'`\n",
    "\n",
    "Once the pipeline is performing well, you can swtich to run it on the full data by setting `ws.debug = False`\n",
    "\n",
    "You can see the effect of the debug mode on the filenames here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stitched.npy\n",
      "debug_stitched.npy\n",
      "test_stitched.npy\n"
     ]
    }
   ],
   "source": [
    "debug = ws.debug;\n",
    "ws.debug = False\n",
    "print(ws.filename('stitched', directory=False))\n",
    "ws.debug = True\n",
    "print(ws.filename('stitched', directory=False))\n",
    "ws.debug = 'test'\n",
    "print(ws.filename('stitched', directory=False))\n",
    "ws.debug = debug;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image processing\n",
    "\n",
    "The goal of the image processing step is to turn the 16bit intensity levels image of vessels and arteries into a “binary” image, in which each pixel either belongs to a vessel or not. This step is essential to get a good quality reconstruction. The center line of the vessels will be evaluated from this binary image, and it necessitates the vessels to appear as filled tubes. Therefore, the image processing has two steps: In the first step, we create the first binary mask from the vessels and arteries images. Then in the second step we clean this binary image by removing holes in the vessels and noise in the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarization - Vasculature\n",
    "\n",
    "The two important parameters to set here are:\n",
    "    \n",
    "* `processing_parameter['processes']` which defines how many processors work at the same time.\n",
    "  \n",
    "  The more processors work, the faster the process will be, but the more system (RAM) memory you need. \n",
    "  You can start with 10 and check if you get a memory error. If you do, you can decrease the number of processors.\n",
    "  If it works fine, you can test with more processors.\n",
    "  \n",
    "  \n",
    "* `binarization_parameter['clip']['clip_range']` as tuple (min, max) sets the range for the vessel detection.\n",
    "\n",
    "  To help you find this range, open the main scan mosaic with the command `ws.plot(‘stitched’)`. Look for the \n",
    "  weakest intensities in the pixels inside the brain (ignore the pixels outside the brain). Set the minimum based \n",
    "  on the lowest intensities you see. For the higher intensity, look at the large vessels at the surface. You \n",
    "  should set to the limit to a value that still excludes the halo.\n",
    "  \n",
    "  ![Clipping](Static/ImageProcessing_clipping.jpg)\n",
    "  \n",
    "  All parameters forthe image processing are described in the [methods](tubemap.rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Tip\n",
    "\n",
    "The lowest signal intensity in the whole brain is often found, counterintuitively, in the hippocampus of \n",
    "in the deep cerebellar nuclei.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 8 blocks with function 'binarize_block'.\n",
      "Processing block 3/8<(0, 0, 3)/(1, 1, 8)> (400, 600, 38)@(400, 600, 300)[(:,:,112:150)]\n",
      "Processing block 2/8<(0, 0, 2)/(1, 1, 8)> (400, 600, 37)@(400, 600, 300)[(:,:,75:112)]\n",
      "...\n",
      "Block (0, 0, 7)> (400, 600, 38)@(400, 600, 38)[(:,:,:)]: Vesselization: elapsed time: 0:00:19.642\n",
      "Block (0, 0, 7)> (400, 600, 38)@(400, 600, 38)[(:,:,:)]: Vesselization: binarization: elapsed time: 0:00:19.655\n",
      "Block (0, 0, 7)> (400, 600, 38)@(400, 600, 38)[(:,:,:)]: Binarization: elapsed time: 0:01:00.961\n",
      "Processing block 7/8<(0, 0, 7)/(1, 1, 8)> (400, 600, 38)@(400, 600, 300)[(:,:,262:300)]: elapsed time: 0:01:01.139\n",
      "Processed 8 blocks with function 'binarize_block': elapsed time: 0:01:01.792\n"
     ]
    }
   ],
   "source": [
    "source = ws.filename('stitched');\n",
    "sink   = ws.filename('binary');\n",
    "io.delete_file(sink)\n",
    "\n",
    "binarization_parameter = vasc.default_binarization_parameter.copy();\n",
    "binarization_parameter['clip']['clip_range'] = (200, 7000)\n",
    "\n",
    "processing_parameter = vasc.default_binarization_processing_parameter.copy();\n",
    "processing_parameter.update(processes = None,\n",
    "                            as_memory = True,\n",
    "                            verbose = True);\n",
    "   \n",
    "vasc.binarize(source, sink, \n",
    "              binarization_parameter=binarization_parameter, \n",
    "              processing_parameter=processing_parameter);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binarization should take around 4-5h for the full data set. \n",
    "\n",
    "You can inspect and overlay the result using the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ClearMap.Visualization.Qt.DataViewer.DataViewer at 0x7f52e87bed70>,\n",
       " <ClearMap.Visualization.Qt.DataViewer.DataViewer at 0x7f52e4cacb90>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3d.plot([source, sink])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will look somthing like\n",
    "\n",
    "![Binarization](Static/ImageProcessing_binarization_check.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a final step the binarized result is postprocessed by filling small holes and smoothing the surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary post processing: initialized.\n",
      "Binary smoothing: initialized!\n",
      "Processing 8 blocks with function 'smooth_by_configuration'.\n",
      "Processing block 5/8<(0, 0, 5)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,178:228)]\n",
      "Processing block 3/8<(0, 0, 3)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,107:157)]\n",
      "Processing block 1/8<(0, 0, 1)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,35:85)]\n",
      "Processing block 2/8<(0, 0, 2)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,71:121)]\n",
      "Processing block 4/8<(0, 0, 4)/(1, 1, 8)> (400, 600, 49)@(400, 600, 300)[(:,:,143:192)]\n",
      "Processing block 0/8<(0, 0, 0)/(1, 1, 8)> (400, 600, 49)@(400, 600, 300)[(:,:,0:49)]\n",
      "Processing block 6/8<(0, 0, 6)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,214:264)]\n",
      "Processing block 7/8<(0, 0, 7)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,250:300)]\n",
      "Processing block 4/8<(0, 0, 4)/(1, 1, 8)> (400, 600, 49)@(400, 600, 300)[(:,:,143:192)]: elapsed time: 0:00:12.093\n",
      "Processing block 1/8<(0, 0, 1)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,35:85)]: elapsed time: 0:00:12.232\n",
      "Processing block 6/8<(0, 0, 6)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,214:264)]: elapsed time: 0:00:12.323\n",
      "Processing block 3/8<(0, 0, 3)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,107:157)]: elapsed time: 0:00:12.819\n",
      "Processing block 7/8<(0, 0, 7)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,250:300)]: elapsed time: 0:00:13.250\n",
      "Processing block 2/8<(0, 0, 2)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,71:121)]: elapsed time: 0:00:13.309\n",
      "Processing block 0/8<(0, 0, 0)/(1, 1, 8)> (400, 600, 49)@(400, 600, 300)[(:,:,0:49)]: elapsed time: 0:00:13.331\n",
      "Processing block 5/8<(0, 0, 5)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,178:228)]: elapsed time: 0:00:13.933\n",
      "Processed 8 blocks with function 'smooth_by_configuration': elapsed time: 0:00:14.421\n",
      "Binary smoothing: done: elapsed time: 0:00:14.422\n",
      "Binary filling: initialized!\n",
      "Binary filling: elapsed time: 0:00:01.564\n",
      "Binary post processing: elapsed time: 0:00:16.139\n"
     ]
    }
   ],
   "source": [
    "source = ws.filename('binary');\n",
    "sink   = ws.filename('binary', postfix='postprocessed');\n",
    "\n",
    "postprocessing_parameter = vasc.default_postprocessing_parameter.copy();\n",
    "#postprocessing_parameter['fill'] = None;\n",
    "\n",
    "postprocessing_processing_parameter = vasc.default_postprocessing_processing_parameter.copy();\n",
    "postprocessing_processing_parameter.update(size_max=100);\n",
    "\n",
    "vasc.postprocess(source, sink, \n",
    "                 postprocessing_parameter=postprocessing_parameter, \n",
    "                 processing_parameter=postprocessing_processing_parameter, \n",
    "                 processes=None, verbose=True)\n",
    "\n",
    "#p3d.plot([[source, sink]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arterial Binarization\n",
    "\n",
    "The binarizatio for the arterial data set is very similar to the vasculature binarization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 8 blocks with function 'binarize_block'.\n",
      "Processing block 2/8<(0, 0, 2)/(1, 1, 8)> (400, 600, 37)@(400, 600, 300)[(:,:,75:112)]\n",
      "...\n",
      "Processing block 7/8<(0, 0, 7)/(1, 1, 8)> (400, 600, 38)@(400, 600, 300)[(:,:,262:300)]: elapsed time: 0:00:29.976\n",
      "Processing block 3/8<(0, 0, 3)/(1, 1, 8)> (400, 600, 38)@(400, 600, 300)[(:,:,112:150)]: elapsed time: 0:00:29.988\n",
      "Processing block 2/8<(0, 0, 2)/(1, 1, 8)> (400, 600, 37)@(400, 600, 300)[(:,:,75:112)]: elapsed time: 0:00:29.991\n",
      "Processing block 0/8<(0, 0, 0)/(1, 1, 8)> (400, 600, 37)@(400, 600, 300)[(:,:,0:37)]: elapsed time: 0:00:29.986\n",
      "Processed 8 blocks with function 'binarize_block': elapsed time: 0:00:30.608\n"
     ]
    }
   ],
   "source": [
    "source = ws.filename('stitched', postfix='arteries');\n",
    "sink   = ws.filename('binary', postfix='arteries');\n",
    "io.delete_file(sink)\n",
    "\n",
    "binarization_parameter = vasc.default_binarization_parameter.copy();\n",
    "binarization_parameter['clip']['clip_range'] = (1000, 8000)\n",
    "binarization_parameter['deconvolve']['threshold'] = 450  \n",
    "binarization_parameter['equalize'] = None;\n",
    "binarization_parameter['vesselize'] = None;\n",
    "\n",
    "processing_parameter = vasc.default_binarization_processing_parameter.copy();\n",
    "processing_parameter.update(processes = 20,\n",
    "                            as_memory = True, verbose=True);\n",
    "\n",
    "vasc.binarize(source, sink, \n",
    "              binarization_parameter=binarization_parameter, \n",
    "              processing_parameter=processing_parameter);\n",
    "\n",
    "#p3d.plot([source, sink])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary post processing: initialized.\n",
      "Binary smoothing: initialized!\n",
      "Processing 8 blocks with function 'smooth_by_configuration'.\n",
      "Processing block 3/8<(0, 0, 3)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,107:157)]\n",
      "Processing block 0/8<(0, 0, 0)/(1, 1, 8)> (400, 600, 49)@(400, 600, 300)[(:,:,0:49)]\n",
      "Processing block 5/8<(0, 0, 5)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,178:228)]\n",
      "Processing block 4/8<(0, 0, 4)/(1, 1, 8)> (400, 600, 49)@(400, 600, 300)[(:,:,143:192)]\n",
      "Processing block 6/8<(0, 0, 6)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,214:264)]\n",
      "Processing block 2/8<(0, 0, 2)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,71:121)]\n",
      "Processing block 7/8<(0, 0, 7)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,250:300)]\n",
      "Processing block 1/8<(0, 0, 1)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,35:85)]\n",
      "Processing block 2/8<(0, 0, 2)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,71:121)]: elapsed time: 0:00:07.873\n",
      "Processing block 7/8<(0, 0, 7)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,250:300)]: elapsed time: 0:00:08.217\n",
      "Processing block 5/8<(0, 0, 5)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,178:228)]: elapsed time: 0:00:08.257\n",
      "Processing block 6/8<(0, 0, 6)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,214:264)]: elapsed time: 0:00:08.270\n",
      "Processing block 1/8<(0, 0, 1)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,35:85)]: elapsed time: 0:00:08.279\n",
      "Processing block 3/8<(0, 0, 3)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,107:157)]: elapsed time: 0:00:08.473\n",
      "Processing block 0/8<(0, 0, 0)/(1, 1, 8)> (400, 600, 49)@(400, 600, 300)[(:,:,0:49)]: elapsed time: 0:00:08.483\n",
      "Processing block 4/8<(0, 0, 4)/(1, 1, 8)> (400, 600, 49)@(400, 600, 300)[(:,:,143:192)]: elapsed time: 0:00:08.590\n",
      "Processed 8 blocks with function 'smooth_by_configuration': elapsed time: 0:00:08.951\n",
      "Binary smoothing: done: elapsed time: 0:00:08.952\n",
      "Binary filling: initialized!\n",
      "Binary filling: elapsed time: 0:00:01.743\n",
      "Binary post processing: elapsed time: 0:00:10.805\n"
     ]
    }
   ],
   "source": [
    "source = ws.filename('binary', postfix='arteries');\n",
    "sink   = ws.filename('binary', postfix='arteries_postprocessed');\n",
    "sink_smooth = ws.filename('binary', postfix='arteries_smoothed');\n",
    "\n",
    "postprocessing_parameter = vasc.default_postprocessing_parameter.copy();\n",
    "\n",
    "postprocessing_processing_parameter = vasc.default_postprocessing_processing_parameter.copy();\n",
    "postprocessing_processing_parameter.update(size_max = 50);\n",
    "\n",
    "vasc.postprocess(source, sink, postprocessing_parameter=postprocessing_parameter, \n",
    "                 processing_parameter=postprocessing_processing_parameter, \n",
    "                 processes=None, verbose=True)\n",
    "\n",
    "#p3d.plot([source, sink])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ClearMap.Visualization.Qt.DataViewer.DataViewer at 0x7f553d6d17d0>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3d.plot([[ws.filename('stitched', postfix='arteries'), \n",
    "           ws.filename('binary', postfix='arteries_postprocessed')]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vessel filling\n",
    "\n",
    "In this step a trained convolutional deep network is applied to fill the holes of hollow vessels in the vasculature and artieres data.\n",
    "\n",
    "Typically, no parameters neeed to be changed.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Note\n",
    "    \n",
    "The arteries are filled separately so the reuslt can be used to label arterial vessles later on.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vessel filling: neural network initialized: elapsed time: 0:00:00.004\n",
      "VesselFillingNetwork(\n",
      "  (conv1): Conv3d(1, 16, kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(3, 3, 3))\n",
      "  (conv2): Conv3d(16, 32, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
      "  (desepconv4): DeSepConv3d(\n",
      "    (depthwise): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32)\n",
      "    (pointwise): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "  )\n",
      "  (convbin1): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (conv3): ConvTranspose3d(32, 16, kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(3, 3, 3))\n",
      "  (conv4): Conv3d(17, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (dropout): Dropout(p=0.15, inplace=False)\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (out_act): Sigmoid()\n",
      "  (upsample): Upsample(scale_factor=2.0, mode=trilinear)\n",
      ")\n",
      "Vessel filling: using cpu\n",
      "Vessel filling: source loaded: elapsed time: 0:00:00.011\n",
      "Vessel filling: processing block 0/24<(0, 0, 0)/(3, 4, 2)> (200, 200, 200)@(400, 600, 300)[(0:200,0:200,0:200)]\n",
      "Vessel filling: loaded data: (1, 200, 200, 200)\n",
      "Vessel filling: resampled data: (1, 1, 200, 200, 200)\n",
      "Vessel filling: network (1, 1, 198, 198, 198)\n",
      "Vessel filling: upsampled (1, 1, 198, 198, 198)\n",
      "Vessel filling: thresholded (1, 1, 198, 198, 198)\n",
      "Vessel filling: result written to (slice(None, 198, None), slice(None, 198, None), slice(None, 198, None))\n",
      "Vessel filling: processing block 0/24<(0, 0, 0)/(3, 4, 2)> (200, 200, 200)@(400, 600, 300)[(0:200,0:200,0:200)]: elapsed time: 0:00:49.336\n",
      "...\n",
      "Vessel filling: elapsed time: 0:11:40.348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Memmap-Source(400, 600, 300)[bool]|F|{/home/ckirst/Programs/ClearMap2/ClearMap/Tests/Data/TubeMap_Example/debug_binary_filled.npy}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = ws.filename('binary', postfix='postprocessed');\n",
    "sink   = ws.filename('binary', postfix='filled');\n",
    "io.delete_file(sink)\n",
    "\n",
    "processing_parameter = vf.default_fill_vessels_processing_parameter.copy();\n",
    "processing_parameter.update(size_max = 200, \n",
    "                            size_min = 'fixed',\n",
    "                            axes = all,\n",
    "                            overlap = 50);                 \n",
    "                            \n",
    "vf.fill_vessels(source, sink, \n",
    "                resample=1, threshold=0.5, cuda=None, \n",
    "                processing_parameter=processing_parameter, verbose=True)\n",
    "\n",
    "#p3d.plot([source, sink]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vessel filling: neural network initialized: elapsed time: 0:00:00.003\n",
      "VesselFillingNetwork(\n",
      "  (conv1): Conv3d(1, 16, kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(3, 3, 3))\n",
      "  (conv2): Conv3d(16, 32, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
      "  (desepconv4): DeSepConv3d(\n",
      "    (depthwise): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32)\n",
      "    (pointwise): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "  )\n",
      "  (convbin1): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (conv3): ConvTranspose3d(32, 16, kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(3, 3, 3))\n",
      "  (conv4): Conv3d(17, 2, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (maxpool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (dropout): Dropout(p=0.15, inplace=False)\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (out_act): Sigmoid()\n",
      "  (upsample): Upsample(scale_factor=2.0, mode=trilinear)\n",
      ")\n",
      "Vessel filling: using cpu\n",
      "Vessel filling: source loaded: elapsed time: 0:00:00.004\n",
      "Vessel filling: processing block 0/1<(0, 0, 0)/(1, 1, 1)> (400, 600, 300)@(400, 600, 300)[(0:400,0:600,0:300)]\n",
      "Vessel filling: loaded data: (1, 400, 600, 300)\n",
      "Vessel filling: resampled data: (1, 1, 200, 300, 150)\n",
      "Vessel filling: network (1, 1, 198, 294, 150)\n",
      "Vessel filling: upsampled (1, 1, 396, 588, 300)\n",
      "Vessel filling: thresholded (396, 588, 300)\n",
      "Vessel filling: result written to (slice(None, 396, None), slice(None, 588, None), slice(None, None, None))\n",
      "Vessel filling: processing block 0/1<(0, 0, 0)/(1, 1, 1)> (400, 600, 300)@(400, 600, 300)[(0:400,0:600,0:300)]: elapsed time: 0:00:37.691\n",
      "Vessel filling: elapsed time: 0:00:37.696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Memmap-Source(400, 600, 300)[bool]|F|{/home/ckirst/Programs/ClearMap2/ClearMap/Tests/Data/TubeMap_Example/debug_binary_arteries_filled.npy}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = ws.filename('binary', postfix='arteries_postprocessed');\n",
    "sink   = ws.filename('binary', postfix='arteries_filled');\n",
    "io.delete_file(sink)\n",
    "\n",
    "processing_parameter = vf.default_fill_vessels_processing_parameter.copy();\n",
    "processing_parameter.update(size_max = 1000, \n",
    "                            size_min = 'fixed',\n",
    "                            axes = all,\n",
    "                            overlap = 100);                 \n",
    "                            \n",
    "vf.fill_vessels(source, sink, \n",
    "                resample=2, threshold=0.5, cuda=None, \n",
    "                processing_parameter=processing_parameter, verbose=True)\n",
    "\n",
    "#p2d.plot([source, sink]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, both images are combined into a single binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 blocks with function 'logical_or'.\n",
      "Processing block 0/1<(0, 0, 0)/(1, 1, 1)> (400, 600, 300)@(400, 600, 300)[(:,:,0:300)]\n",
      "Processing block 0/1<(0, 0, 0)/(1, 1, 1)> (400, 600, 300)@(400, 600, 300)[(:,:,0:300)]: elapsed time: 0:00:00.935\n",
      "Processed 1 blocks with function 'logical_or': elapsed time: 0:00:01.216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ckirst/Programs/ClearMap2/ClearMap/Tests/Data/TubeMap_Example/debug_binary_combined.npy'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source          = ws.filename('binary', postfix='filled');\n",
    "source_arteries = ws.filename('binary', postfix='arteries_filled');\n",
    "sink            = ws.filename('binary', postfix='combined');\n",
    "\n",
    "io.delete_file(sink)\n",
    "\n",
    "bp.process(np.logical_or, [source, source_arteries], sink, \n",
    "           size_max=500, overlap=0, processes=None, verbose=True)\n",
    "\n",
    "#p3d.plot([[source, source_arteries, sink]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a standard binary smoothing and filling is used to fill any remaining holes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary post processing: initialized.\n",
      "Binary smoothing: initialized!\n",
      "Processing 8 blocks with function 'smooth_by_configuration'.\n",
      "Processing block 0/8<(0, 0, 0)/(1, 1, 8)> (400, 600, 49)@(400, 600, 300)[(:,:,0:49)]\n",
      "Processing block 4/8<(0, 0, 4)/(1, 1, 8)> (400, 600, 49)@(400, 600, 300)[(:,:,143:192)]\n",
      "Processing block 1/8<(0, 0, 1)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,35:85)]\n",
      "Processing block 5/8<(0, 0, 5)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,178:228)]\n",
      "Processing block 3/8<(0, 0, 3)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,107:157)]\n",
      "Processing block 2/8<(0, 0, 2)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,71:121)]\n",
      "Processing block 6/8<(0, 0, 6)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,214:264)]\n",
      "Processing block 7/8<(0, 0, 7)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,250:300)]\n",
      "Processing block 1/8<(0, 0, 1)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,35:85)]: elapsed time: 0:00:08.295\n",
      "Processing block 6/8<(0, 0, 6)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,214:264)]: elapsed time: 0:00:08.441\n",
      "Processing block 0/8<(0, 0, 0)/(1, 1, 8)> (400, 600, 49)@(400, 600, 300)[(:,:,0:49)]: elapsed time: 0:00:08.719\n",
      "Processing block 5/8<(0, 0, 5)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,178:228)]: elapsed time: 0:00:08.765\n",
      "Processing block 2/8<(0, 0, 2)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,71:121)]: elapsed time: 0:00:08.772\n",
      "Processing block 3/8<(0, 0, 3)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,107:157)]: elapsed time: 0:00:08.797\n",
      "Processing block 7/8<(0, 0, 7)/(1, 1, 8)> (400, 600, 50)@(400, 600, 300)[(:,:,250:300)]: elapsed time: 0:00:08.800\n",
      "Processing block 4/8<(0, 0, 4)/(1, 1, 8)> (400, 600, 49)@(400, 600, 300)[(:,:,143:192)]: elapsed time: 0:00:08.923\n",
      "Processed 8 blocks with function 'smooth_by_configuration': elapsed time: 0:00:09.197\n",
      "Binary smoothing: done: elapsed time: 0:00:09.199\n",
      "Binary filling: initialized!\n",
      "Binary filling: elapsed time: 0:00:01.428\n",
      "Binary post processing: elapsed time: 0:00:10.715\n"
     ]
    }
   ],
   "source": [
    "source = ws.filename('binary', postfix='combined');\n",
    "sink   = ws.filename('binary', postfix='final');\n",
    "\n",
    "postprocessing_parameter = vasc.default_postprocessing_parameter.copy();\n",
    "postprocessing_processing_parameter = vasc.default_postprocessing_processing_parameter.copy();\n",
    "postprocessing_processing_parameter['size_max'] = 50;\n",
    "\n",
    "vasc.postprocess(source, sink, postprocessing_parameter=postprocessing_parameter, \n",
    "                 processing_parameter=postprocessing_processing_parameter, \n",
    "                 processes=None, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph construction\n",
    "\n",
    "In this section the binary image is turned into a graph with 3d geometric information of the vasculatur network as well as marker expression levels and atal annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skeletonization\n",
    "\n",
    "The first step will create a vessel center line and for a full data set.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Note\n",
    "    \n",
    "The skeletonizatoin algorithm is highly optimized. While the algorithm here should take about 1h for a hemisphere, standard algorithms can take over a week of processing.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################################################\n",
      "Skeletonization PK12 [convolution, index]\n",
      "Foreground points: 7902209: elapsed time: 0:00:00.076\n",
      "#############################################################\n",
      "Iteration 1\n",
      "<class 'numpy.ndarray'> int64 bool\n",
      "uint8 int64 uint8 int64 int64 uint8\n",
      "Border points: 2787009: elapsed time: 0:00:00.195\n",
      "-------------------------------------------------------------\n",
      "Sub-Iteration 0\n",
      "Matched points  : 549415: elapsed time: 0:00:00.613\n",
      "Sub-Iteration 0: elapsed time: 0:00:00.633\n",
      "-------------------------------------------------------------\n",
      "Sub-Iteration 1\n",
      "Matched points  : 574686: elapsed time: 0:00:00.153\n",
      "Sub-Iteration 1: elapsed time: 0:00:00.179\n",
      "...\n",
      "-------------------------------------------------------------\n",
      "Sub-Iteration 9\n",
      "Matched points  : 0: elapsed time: 0:00:00.000\n",
      "Sub-Iteration 9: elapsed time: 0:00:00.000\n",
      "-------------------------------------------------------------\n",
      "Sub-Iteration 10\n",
      "Matched points  : 0: elapsed time: 0:00:00.000\n",
      "Sub-Iteration 10: elapsed time: 0:00:00.000\n",
      "-------------------------------------------------------------\n",
      "Sub-Iteration 11\n",
      "Matched points  : 0: elapsed time: 0:00:00.000\n",
      "Sub-Iteration 11: elapsed time: 0:00:00.000\n",
      "-------------------------------------------------------------\n",
      "Non-removable points: 4\n",
      "Foreground points   : 7968\n",
      "-------------------------------------------------------------\n",
      "Iteration 27: elapsed time: 0:00:00.022\n",
      "#############################################################\n",
      "Skeletonization done: elapsed time: 0:00:07.966\n",
      "Total removed:   7767404\n",
      "Total remaining: 134805\n",
      "Skeletonization: elapsed time: 0:00:08.049\n"
     ]
    }
   ],
   "source": [
    "binary   = ws.filename('binary', postfix='filled');\n",
    "skeleton = ws.filename('skeleton')                   \n",
    "\n",
    "skl.skeletonize(binary, sink=skeleton, delete_border=True, verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ClearMap.Visualization.Qt.DataViewer.DataViewer at 0x7f300404a4b0>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary   = ws.filename('binary', postfix='filled');\n",
    "skeleton = ws.filename('skeleton')    \n",
    "p3d.plot([[binary, skeleton]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ImageProcessinh Skeletonization](Static/ImageProcessing_skeletonization.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw graph construction\n",
    "\n",
    "The raw graph consists of nodes at each center pixel of the skeleton and edges between neighbouring pixels \n",
    "(26-connected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph from skeleton calculation initialize.!\n",
      "Point list generation: elapsed time: 0:00:00.056\n",
      "Graph initialized with 134805 vertices: elapsed time: 0:00:00.067\n",
      "11928 edges with orientation 1/13 found: elapsed time: 0:00:00.001\n",
      "9371 edges with orientation 2/13 found: elapsed time: 0:00:00.001\n",
      "11104 edges with orientation 3/13 found: elapsed time: 0:00:00.000\n",
      "9943 edges with orientation 4/13 found: elapsed time: 0:00:00.000\n",
      "15953 edges with orientation 5/13 found: elapsed time: 0:00:00.008\n",
      "14305 edges with orientation 6/13 found: elapsed time: 0:00:00.022\n",
      "7887 edges with orientation 7/13 found: elapsed time: 0:00:00.016\n",
      "13859 edges with orientation 8/13 found: elapsed time: 0:00:00.023\n",
      "15441 edges with orientation 9/13 found: elapsed time: 0:00:00.023\n",
      "9484 edges with orientation 10/13 found: elapsed time: 0:00:00.022\n",
      "7666 edges with orientation 11/13 found: elapsed time: 0:00:00.019\n",
      "8023 edges with orientation 12/13 found: elapsed time: 0:00:00.023\n",
      "6902 edges with orientation 13/13 found: elapsed time: 0:00:00.010\n",
      "Added 141866 edges to graph: elapsed time: 0:00:00.032\n",
      "Skeleton to Graph: elapsed time: 0:00:00.358\n"
     ]
    }
   ],
   "source": [
    "graph_raw = gp.graph_from_skeleton(ws.filename('skeleton'), verbose=True)\n",
    "#graph_raw.save(ws.filename('graph', postfix='raw'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<GraphLine at 0x7f2ffcc63f10>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3d.plot_graph_line(graph_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Grah raw](Static/GraphConstruction_raw_line.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurements\n",
    "\n",
    "Nex, measurements of expressoin levels of the markers and geometric properties are \n",
    "performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring radii of 134805 points in array of shape (400, 600, 300).\n",
      "find_smaller_than_value: initialized!\n",
      "find_smaller_than_value: elapsed time: 0:00:00.153\n",
      "Measuring radii done: elapsed time: 0:00:10.356\n"
     ]
    }
   ],
   "source": [
    "# Measure radii\n",
    "\n",
    "coordinates = graph_raw.vertex_coordinates();   \n",
    "radii, indices = mr.measure_radius(ws.filename('binary', postfix='filled'), coordinates, \n",
    "                                   value=0, fraction=None, max_radius=150, \n",
    "#                                   value=None, fraction=0.8, max_radius=150,\n",
    "                                   return_indices=True, default=-1, verbose=True);  \n",
    "graph_raw.set_vertex_radii(radii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring expression of 134805 points in array of shape (400, 600, 300).\n",
      "measure_max: initialized!\n",
      "measure_max: elapsed time: 0:00:00.059\n",
      "Measuring expression done: elapsed time: 0:00:00.068\n"
     ]
    }
   ],
   "source": [
    "# Artery binary measure\n",
    "\n",
    "binary_arteries = ws.filename('binary', postfix='arteries_filled');\n",
    "\n",
    "coordinates = graph_raw.vertex_coordinates();\n",
    "radii = graph_raw.vertex_radii();\n",
    "radii_measure = radii + 10;\n",
    "\n",
    "expression = me.measure_expression(binary_arteries, coordinates, radii, \n",
    "                                   method='max', verbose=True);\n",
    "\n",
    "graph_raw.define_vertex_property('artery_binary', expression);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring expression of 134805 points in array of shape (400, 600, 300).\n",
      "measure_max: initialized!\n",
      "measure_max: elapsed time: 0:00:02.393\n",
      "Measuring expression done: elapsed time: 0:00:02.420\n"
     ]
    }
   ],
   "source": [
    "# Artery raw measure\n",
    "\n",
    "artery_raw = ws.filename('stitched', postfix='arteries');\n",
    "\n",
    "coordinates = graph_raw.vertex_coordinates();\n",
    "radii = graph_raw.vertex_radii();\n",
    "radii_measure = radii + 10;\n",
    "\n",
    "expression = me.measure_expression(artery_raw, coordinates, radii_measure, \n",
    "                                   method='max', verbose=True);\n",
    "\n",
    "graph_raw.define_vertex_property('artery_raw', np.asarray(expression.array, dtype=float));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw graph\n",
    "\n",
    "graph_raw.save(ws.filename('graph', postfix='raw'))\n",
    "#graph_raw = grp.load(ws.filename('graph', postfix='raw'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph postprocessing\n",
    "\n",
    "In this step the raw graph is cleaned and simplified to a reduced graph, in which nodes and edges forming a single  are reduced to a single edge between branching nodes. The full goemetric information of each branch and expressoin levels are stored in reduced graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph cleaning: found 10110 branch points among 134805 vertices: elapsed time: 0:00:00.002\n",
      "Graph cleaning: detected 1895 cliques of branch points: elapsed time: 0:00:00.121\n",
      "Graph cleaning: removed 1895 cliques of branch points from 134805 to 128565 nodes and 141866 to 129899 edges: elapsed time: 0:00:00.587\n",
      "Graph cleaning: Removed 53 isolated nodes: elapsed time: 0:00:00.025\n",
      "Graph cleaning: cleaned graph has 128512 nodes and 129899 edges: elapsed time: 0:00:00.737\n"
     ]
    }
   ],
   "source": [
    "# Graph cleaning \n",
    "graph_cleaned = gp.clean_graph(graph_raw, \n",
    "                               vertex_mappings = {'coordinates'   : gp.mean_vertex_coordinates, \n",
    "                                                  'radii'         : np.max,\n",
    "                                                  'artery_binary' : np.max,\n",
    "                                                  'artery_raw'    : np.max},                    \n",
    "                               verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned graph\n",
    "\n",
    "graph_cleaned.save(ws.filename('graph', postfix='cleaned'))\n",
    "#graph_cleaned = grp.load(ws.filename('graph', postfix='cleaned'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph reduction: initialized.\n",
      "Graph reduction: Found 5447 branching and 123065 non-branching nodes: elapsed time: 0:00:00.080\n",
      "Graph reduction: Scanned 5447/5447 branching nodes, found 254 branches: elapsed time: 0:00:00.270\n",
      "Graph reduction: Scanned 123065/123065 non-branching nodes found 6834 branches: elapsed time: 0:00:02.644\n",
      "Graph reduction: Graph reduced from 128512 to 5447 nodes and 129899 to 6834 edges: elapsed time: 0:00:02.799\n"
     ]
    }
   ],
   "source": [
    "# Graph reduction\n",
    "\n",
    "def vote(expression):\n",
    "  return np.sum(expression) >= len(expression) / 1.5;\n",
    "\n",
    "graph_reduced = gp.reduce_graph(graph_cleaned, edge_length=True,\n",
    "                          edge_to_edge_mappings = {'length' : np.sum},\n",
    "                          vertex_to_edge_mappings={'artery_binary' : vote,\n",
    "                                                   'artery_raw'    : np.max,\n",
    "                                                   'radii'         : np.max},  \n",
    "                          edge_geometry_vertex_properties=['coordinates', 'radii', 'artery_binary', 'artery_raw'],\n",
    "                          edge_geometry_edge_properties=None,                        \n",
    "                          return_maps=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save reduced graph\n",
    "\n",
    "graph_reduced.save(ws.filename('graph', postfix='reduced'))\n",
    "#graph_reduced = grp.load(ws.filename('graph', postfix='reduced'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize graph annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<GraphLine at 0x7f2f6a7b3110>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artery_raw = graph_reduced.edge_property('artery_raw')\n",
    "edge_colors = p3d.col.color_map('viridis')(artery_raw/artery_raw.max());\n",
    "p3d.plot_graph_line(graph_reduced, edge_colors=edge_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<GraphLine at 0x7f2f6a6605d0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radii = np.array(graph_reduced.edge_property('radii'), dtype=int)\n",
    "radii_colors = p3d.col.color_map('magma')(radii/radii.max());\n",
    "p3d.plot_graph_line(graph_reduced, edge_colors = radii_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ImageProcessing Graph annotation](Static/ImageProcessing_graph_line_annotation.jpg)\n",
    "\n",
    "You can navigate through the graph via the mouse holding the following keys\n",
    "\n",
    "| Mouse button   | Keys    | Navigation              |\n",
    "|----------------|---------|-------------------------|\n",
    "| `left`         |         | 3d rotate around center |\n",
    "| `left`         | `Ctrl`  | roll rotation           |\n",
    "| `left`         | `Meta`  | elevation rotation      |\n",
    "| `left`         | `Shift` | azimuth rotation        |\n",
    "| `right`        |         | zoom                    |\n",
    "| `right`        | `Shift` | translate center        |\n",
    "| `right`        | `Meta`  | field of view           |\n",
    "| `wheel`        |         | zoom                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rendering graphs as with full mesh give more infomration about the geometry but also takes more processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<GraphMesh at 0x7f2f8a9342d0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3d.plot_graph_mesh(graph_reduced, edge_colors=radii_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ImageProcessing Graph Mesh](Static/ImageProcessing_graph_mesh_annotation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For rendering the graph the meshes can also be simplified and smoothed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mesh interpolation 0 / 6834.\n",
      "Mesh interpolation 1000 / 6834.\n",
      "Mesh interpolation 2000 / 6834.\n",
      "Mesh interpolation 3000 / 6834.\n",
      "Mesh interpolation 4000 / 6834.\n",
      "Mesh interpolation 6000 / 6834.\n",
      "Mesh interpolation 5000 / 6834.\n",
      "Mesh calculation 0 / 6834.\n",
      "Mesh calculation 1000 / 6834.\n",
      "Mesh calculation 3000 / 6834.\n",
      "Mesh calculation 2000 / 6834.\n",
      "Mesh calculation 5000 / 6834.\n",
      "Mesh calculation 4000 / 6834.\n",
      "Mesh calculation 6000 / 6834.\n"
     ]
    }
   ],
   "source": [
    "import ClearMap.Analysis.Graphs.GraphRendering as gr\n",
    "\n",
    "interpolation = gr.interpolate_edge_geometry(graph_reduced, smooth=5, order=2, points_per_pixel=0.5, verbose=True);\n",
    "\n",
    "coordinates, faces, colors = gr.mesh_tube_from_coordinates_and_radii(*interpolation, \n",
    "                                    n_tube_points=8, edge_colors=radii_colors, \n",
    "                                    processes=None, verbose=True);\n",
    "                                                                  \n",
    "p = p3d.plot_mesh_3d(coordinates, faces, vertex_colors=colors);  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ImageProcessing Graph Mesh Interpolation](Static/ImageProcessing_graph_mesh_interpolation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph atlas registration and annotation\n",
    "\n",
    "The folloiwng transdforms the graphs coordinates accoridng to the atlas registration calciulated from the raw images earlier and annotated it with the atlas label as well as a distance to brain surface measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph atlas registration\n",
    "\n",
    "def transformation(coordinates):\n",
    "  coordinates = res.resample_points(\n",
    "                  coordinates, sink=None, orientation=None, \n",
    "                  source_shape=io.shape(ws.filename('binary', postfix='final')), \n",
    "                  sink_shape=io.shape(ws.filename('resampled')));\n",
    "  \n",
    "  coordinates = elx.transform_points(\n",
    "                  coordinates, sink=None, \n",
    "                  transform_directory=ws.filename('resampled_to_auto'), \n",
    "                  binary=True, indices=False);\n",
    "  \n",
    "  coordinates = elx.transform_points(\n",
    "                  coordinates, sink=None, \n",
    "                  transform_directory=ws.filename('auto_to_reference'),\n",
    "                  binary=True, indices=False);\n",
    "      \n",
    "  return coordinates;\n",
    "\n",
    "graph_reduced.transform_properties(transformation=transformation, \n",
    "                           vertex_properties = {'coordinates' : 'coordinates_atlas'},\n",
    "                           edge_geometry_properties = {'coordinates' : 'coordinates_atlas'},\n",
    "                           verbose=True);\n",
    "\n",
    "\n",
    "def scaling(radii):\n",
    "  resample_factor = res.resample_factor(\n",
    "                      source_shape=io.shape(ws.filename('binary', postfix='final')), \n",
    "                      sink_shape=io.shape(ws.filename('resampled')))\n",
    "  return radii * np.mean(resample_factor);\n",
    "\n",
    "\n",
    "graph_reduced.transform_properties(transformation=scaling,\n",
    "                           vertex_properties = {'radii' : 'radii_atlas'},\n",
    "                           edge_properties   = {'radii' : 'radii_atlas'},\n",
    "                           edge_geometry_properties = {'radii' : 'radii_atlas'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph atlas annotation\n",
    "\n",
    "ano.set_annotation_file(annotation_file)\n",
    "def annotation(coordinates):\n",
    "  label = ano.label_points(coordinates, key='order');\n",
    "  return label;\n",
    "\n",
    "graph_reduced.annotate_properties(annotation, \n",
    "                          vertex_properties = {'coordinates_atlas' : 'annotation'},\n",
    "                          edge_geometry_properties = {'coordinates_atlas' : 'annotation'});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance to surface\n",
    "\n",
    "distance_atlas = io.as_source(distance_file)\n",
    "distance_atlas_shape = distance_atlas.shape\n",
    "\n",
    "def distance(coordinates):\n",
    "  c = np.asarray(np.round(coordinates), dtype=int);\n",
    "  c[c<0] = 0;\n",
    "  x = c[:,0]; y = c[:,1]; z = c[:,2];\n",
    "  x[x>=distance_atlas_shape[0]] = distance_atlas_shape[0]-1;\n",
    "  y[y>=distance_atlas_shape[1]] = distance_atlas_shape[1]-1;\n",
    "  z[z>=distance_atlas_shape[2]] = distance_atlas_shape[2]-1;\n",
    "  d = distance_atlas[x,y,z];\n",
    "  return d;\n",
    "\n",
    "graph_reduced.transform_properties(distance, \n",
    "                           vertex_properties = {'coordinates_atlas' : 'distance_to_surface'},\n",
    "                           edge_geometry_properties = {'coordinates_atlas' : 'distance_to_surface'});\n",
    "\n",
    "distance_to_surface = graph_reduced.edge_geometry('distance_to_surface', as_list=True);   \n",
    "distance_to_surface__edge = np.array([np.min(d) for d in distance_to_surface])                 \n",
    " \n",
    "graph_reduced.define_edge_property('distance_to_surface', distance_to_surface__edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we remove all the isolated components and keep the largest graph component only and save the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph largests component\n",
    "\n",
    "graph = graph_reduced.largest_component()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save annotated graph\n",
    "\n",
    "graph.save(ws.filename('graph', postfix='annotated'))\n",
    "#graph = grp.load(ws.filename('graph', postfix='annotated'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artery & vein processing\n",
    "\n",
    "In this section we label arteries and veins making use of the vessels radii as well as the ACTA2 expression.\n",
    "\n",
    "\n",
    "### Large vein detection\n",
    "\n",
    "To detect the large veins, we use two criteria: a large radius and low expression of the Acta2 marker.\n",
    "\n",
    "This is visible in the plot below, where each dot represents an annotated vessel, its  Acta2 signal on the x-axis and its radius on the y-axis. Blacks are capillaries, blues veins and reds arteries. Large veins have a low Acta2 expression predominantly, while arteries high Acta2 expression. \n",
    "\n",
    "![Artery and Vein distribution](Static/Artery_Vein_distribution.png)\n",
    "\n",
    "We first look at a “safe” quadrant to identify large veins and pick a high radius and a low maximal Acta2 expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# veins: large radii and low acta2 expression\n",
    "vein_large_radius = 8\n",
    "vein_artery_expression_min = 0;\n",
    "vein_artery_expression_max = 2500;\n",
    "\n",
    "radii  = graph.edge_property('radii');\n",
    "artery_expression = graph.edge_property('artery_raw');\n",
    "\n",
    "vessel_large  = radii >=  vein_large_radius;\n",
    "\n",
    "vein_expression = np.logical_and(artery_expression >= vein_artery_expression_min, \n",
    "                                 artery_expression <= vein_artery_expression_max);\n",
    "\n",
    "vein_large = np.logical_and(vessel_large, vein_expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artery detection\n",
    "\n",
    "Arteries have been defined previously as vessels that have Acta2 expression in the binary mask as a first approximation.\n",
    "\n",
    "To correct for errors due to isolated vessel branches or large veins that express low levels of Acta2,\n",
    "we require a minimum number `min_artery_size` of branches an artery must have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arteries\n",
    "\n",
    "min_artery_size = 3;\n",
    "\n",
    "artery = graph.edge_property('artery_binary');\n",
    "graph_artery = graph.sub_graph(edge_filter=artery, view=True);\n",
    "graph_artery_edge, edge_map = graph_artery.edge_graph(return_edge_map=True)\n",
    "\n",
    "artery_components, artery_size = graph_artery_edge.label_components(return_vertex_counts=True);\n",
    "remove = edge_map[np.in1d(artery_components, np.where(artery_size < min_artery_size)[0])];\n",
    "artery[remove] = False;\n",
    "\n",
    "artery = np.logical_and(artery, np.logical_not(vein_large))\n",
    "\n",
    "graph.define_edge_property('artery', artery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To correct the lableing further, arteries are traced to smaller radii and expression levels.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Note\n",
    "    \n",
    "Vessels on the cortical surface often cross and thus might not fully get resolved as separate vessels. To avoid tracing in those cases, we stop the tracing when its approaching the surface.\n",
    "</div>\n",
    "\n",
    "The parameters are:\n",
    "\n",
    "* `artery_trace_radius` is the radius (in pixel) at which the traicng is stopped\n",
    "\n",
    "* `artery_expression_min` is the minimum signal level at which tracing is stopped\n",
    "\n",
    "* `distance_threshold` is the minimum distance to the surface (in pixel) at whihc tracing is stopped \n",
    "  (15 pixels in the Atlas coordinate system are about 375µm)\n",
    "  \n",
    "* `max_artery_tracing` is the maximum umber of branches to add to the tracing.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Tip\n",
    "    \n",
    "You can define your own conditions that will continue the tracing via changing the `continue_edge` function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# artery tracing\n",
    "\n",
    "#stop at surface, vein or low artery expression\n",
    "artery_trace_radius = 5;\n",
    "artery_expression_min = 500;\n",
    "distance_threshold = 15;\n",
    "max_artery_tracing = 5;\n",
    "\n",
    "radii = graph.edge_property('radii');\n",
    "artery = graph.edge_property('artery');\n",
    "artery_expression = graph.edge_property('artery_raw');\n",
    "distance_to_surface = graph.edge_property('distance_to_surface');\n",
    "\n",
    "def continue_edge(graph, edge):\n",
    "  if distance_to_surface[edge] < distance_threshold or vein[edge]:\n",
    "    return False;\n",
    "  else:\n",
    "    return radii[edge] >= artery_trace_radius and artery_expression[edge] >= artery_expression_min;\n",
    "\n",
    "artery_traced = gp.trace_edge_label(graph, artery, condition=continue_edge, max_iterations=max_artery_tracing);\n",
    "\n",
    "# optional binary opening or closing\n",
    "#artery_traced = graph.edge_close_binary(artery_traced, steps=1);\n",
    "#artery_traced = graph.edge_open_binary(artery_traced, steps=1);\n",
    "\n",
    "graph.define_edge_property('artery', artery_traced);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vein detection\n",
    "\n",
    "In this step we detect big unlabled vessels as veins and also trace those through the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veins - big\n",
    "\n",
    "vein_big_radius = 6\n",
    "\n",
    "radii  = graph.edge_property('radii');\n",
    "artery = graph.edge_property('artery');\n",
    "big_vessel  = radii >=  vein_big_radius;\n",
    "\n",
    "vein = np.logical_and(np.logical_or(vein_large,big_vessel), np.logical_not(artery))\n",
    "\n",
    "graph.define_edge_property('vein_big', vein); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veins - tracing \n",
    "\n",
    "#trace veins by hysteresis thresholding - stop before arteries\n",
    "vein_trace_radius = 5;\n",
    "max_vein_tracing = 5;\n",
    "min_distance_to_artery = 1;\n",
    "\n",
    "radii = graph.edge_property('radii');\n",
    "artery = graph.edge_property('artery');\n",
    "vein_big  = graph.edge_property('vein_big');\n",
    "\n",
    "artery_expanded = graph.edge_dilate_binary(artery, steps=min_distance_to_artery);\n",
    "\n",
    "def continue_edge(graph, edge):\n",
    "  if artery_expanded[edge]:\n",
    "    return False;\n",
    "  else:\n",
    "    return radii[edge] >= vein_trace_radius;\n",
    "\n",
    "vein = gp.trace_edge_label(graph, vein_big, condition=continue_edge, max_iterations=max_vein_tracing);\n",
    "\n",
    "#vein = graph.edge_close_binary(vein, steps=1);\n",
    "#vein = graph.edge_open_binary(vein, steps=1);\n",
    "\n",
    "graph.define_edge_property('vein', vein);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artery and vein detection - filtering\n",
    "\n",
    "In this final optional step we remove artery and vein components smaller than a certain size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arteries - remove small components \n",
    "\n",
    "min_artery_size = 30;\n",
    "\n",
    "artery = graph.edge_property('artery');\n",
    "graph_artery = graph.sub_graph(edge_filter=artery, view=True);\n",
    "graph_artery_edge, edge_map = graph_artery.edge_graph(return_edge_map=True)\n",
    "\n",
    "artery_components, artery_size = graph_artery_edge.label_components(return_vertex_counts=True);\n",
    "remove = edge_map[np.in1d(artery_components, np.where(artery_size < min_artery_size)[0])];\n",
    "artery[remove] = False;\n",
    "\n",
    "graph.define_edge_property('artery', artery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veins - remove small vein components \n",
    "\n",
    "min_vein_size = 30;\n",
    "\n",
    "vein = graph.edge_property('vein');\n",
    "graph_vein = graph.sub_graph(edge_filter=vein, view=True);\n",
    "graph_vein_edge, edge_map = graph_vein.edge_graph(return_edge_map=True)\n",
    "\n",
    "vein_components, vein_size = graph_vein_edge.label_components(return_vertex_counts=True);\n",
    "remove = edge_map[np.in1d(vein_components, np.where(vein_size < min_vein_size)[0])];\n",
    "vein[remove] = False;\n",
    "\n",
    "graph.define_edge_property('vein', vein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save graph\n",
    "\n",
    "graph.save(ws.filename('graph'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph analysis\n",
    "\n",
    "The final graph can now be analyzed and visualized for inspection.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Tip\n",
    "    \n",
    "To analyze or visualize the graph it can be useful to extract sub-graphs via the `sub_graph` function or slice the graph in space first via `slice_graph`.\n",
    "    \n",
    "You can use the annotation to extract sub-graphs in different brain regions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load graph\n",
    "graph = grp.load(ws.filename('graph'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Isocortex\n",
       "=========\n",
       "id                 : 315\n",
       "atlas_id           : 746\n",
       "ontology_id        : 1\n",
       "acronym            : Isocortex\n",
       "name               : Isocortex\n",
       "color_hex_triplet  : 70FF71\n",
       "graph_order        : 5\n",
       "st_level           : None\n",
       "hemisphere_id      : 3\n",
       "parent_structure_id: 695\n",
       "level              : 6\n",
       "order              : 6\n",
       "rgb                : [0.43921569 1.         0.44313725]\n",
       "color_order        : 6"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ano.find('Isocortex', key='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub-graph extraction\n",
    "label = graph.vertex_annotation();\n",
    "label_leveled = ano.convert_label(label, key='order', value='order', level=6)\n",
    "vertex_filter = label_leveled == 6;\n",
    "\n",
    "gs = graph.sub_graph(vertex_filter=vertex_filter);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Tip\n",
    "\n",
    "You can include neighbouring vertices and edges around the extracted sub-region by expanding a vertex or edge filter several steps into the graph:\n",
    "    \n",
    "`vertex_filter = graph.expand_vertex_filter(vertex_filter, steps=2)`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also simply slice the graph manually, e.g in a sagittal plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = graph.sub_slice((slice(1,300), slice(50,480), slice(165,175)), coordinates='coordinates_atlas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or blocks around other areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub-slice into brain regions\n",
    "\n",
    "# Cerebellum\n",
    "gs = graph.sub_slice((slice(1,270), slice(1,220), slice(210,220)), \n",
    "                   coordinates='coordinates_atlas');\n",
    "\n",
    "# Hippocampus sagittal\n",
    "gs = graph.sub_slice((slice(1,300), slice(50,480), slice(165,185)),\n",
    "                   coordinates='coordinates_atlas');\n",
    "\n",
    "gs = graph.sub_slice((slice(18,180), slice(150,280), slice(153,180)), \n",
    "                   coordinates='coordinates_atlas’);\n",
    "\n",
    "# Striatum coronal\n",
    "gs = graph.sub_slice((slice(1,270), slice(100,108), slice(1,240)),\n",
    "                   coordinates='coordinates_atlas');\n",
    "\n",
    "# Auditory coronal\n",
    "gs = graph.sub_slice((slice(1,270), slice(200,210), slice(1,240)),\n",
    "                   coordinates='coordinates_atlas');\n",
    "                  \n",
    "# Cortex saggittal hippocampus\n",
    "gs = graph.sub_slice((slice(1,300), slice(270,280), slice(1,240)),\n",
    "                   coordinates='coordinates_atlas');\n",
    "\n",
    "# Midline\n",
    "gs = graph.sub_slice((slice(500,1500), slice(3000,4000), slice(2910,2960)));  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph visualization\n",
    "\n",
    "Here are some examples how to visualize various aspects of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line graph - ABA colors\n",
    "\n",
    "vertex_colors = ano.convert_label(gs.vertex_annotation(), key='order', value='rgba');\n",
    "p = p3d.plot_graph_line(gs, color=vertex_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% mesh graph - ABA colors\n",
    "\n",
    "vertex_colors = ano.convert_label(gs.vertex_annotation(), key='order', value='rgba');\n",
    "p = p3d.plot_graph_mesh(gs, default_radius=0.15, vertex_colors=vertex_colors, n_tube_points=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_vein_label = gs.edge_property('vein');\n",
    "edge_artery_label = gs.edge_property('artery');\n",
    "\n",
    "vertex_colors = ano.convert_label(gs.vertex_annotation(), key='order', value='rgba');\n",
    "\n",
    "connectivity = gs.edge_connectivity();\n",
    "edge_colors = (vertex_colors[connectivity[:,0]] + vertex_colors[connectivity[:,1]])/2.0;\n",
    "edge_colors[edge_artery_label >0] = [0.8,0.0,0.0,1.0]\n",
    "edge_colors[edge_vein_label  >0] = [0.0,0.0,0.8,1.0]\n",
    "\n",
    "p = p3d.plot_graph_mesh(gs, edge_colors=edge_colors, n_tube_points=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ImageProcessing Graph Slice](Static/ImageProcessing_graph_slice.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veins and arteries only\n",
    "\n",
    "edge_vein_label = gs.edge_property('vein');\n",
    "edge_artery_label = gs.edge_property('artery')\n",
    "\n",
    "edge_filter=np.logical_or(edge_vein_label,edge_artery_label)\n",
    "gsrt = gs.sub_graph(edge_filter=edge_filter)\n",
    "\n",
    "edge_vein_label = gsrt.edge_property('vein');\n",
    "edge_artery_label = gsrt.edge_property('artery')\n",
    "\n",
    "edge_colors = np.zeros((grst.n_edges, 4));\n",
    "edge_colors[edge_artery_label] = [0.8,0.0,0.0,1.0];\n",
    "edge_colors[edge_vein_label  ] = [0.0,0.0,0.8,1.0];\n",
    "\n",
    "p = p3d.plot_graph_mesh(gsrt, edge_colors=edge_colors, n_tube_points=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ImageProcessing Graph Arteries Veins](Static/ImageProcessing_graph_arteries_veins.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vessel orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vessel orientation \n",
    "\n",
    "vetex_coordinates = gs.vertex_coordinates()\n",
    "connectivity = gs.edge_connectivity();\n",
    "\n",
    "orientations = vetex_coordinates[connectivity[:,0]] - vetex_coordinates[connectivity[:,1]];\n",
    "orientations = (orientations.T / np.linalg.norm(orientations, axis=1)).T\n",
    "\n",
    "#edge_colors = col.orientation_to_rgb(orientations, alpha=1.0);\n",
    "edge_colors = col.orientation_to_boys(orientations, alpha=1.0);\n",
    "edge_artery_label = gs.edge_property('artery');\n",
    "edge_colors[edge_artery_label>0] = [0.8,0.0,0.0,1.0]\n",
    "\n",
    "p = p3d.plot_graph_mesh(gs, edge_colors=edge_colors, n_tube_points=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Branch density\n",
    "\n",
    "A simple way to calculate densites of graph properties is via the voxelization tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxelize_branch_parameter = {\n",
    "    \"method\"  : 'sphere',      \n",
    "    \"radius\"  : (15,15,15),\n",
    "    \"weights\" : None,\n",
    "    \"shape\"   : io.shape(reference_file),\n",
    "    \"verbose\" : True                  \n",
    "};\n",
    "\n",
    "vertices = graph.vertex_coordinates();    \n",
    "    \n",
    "branch_density = vox.voxelize(vertices, sink=ws.filename('density', postfix='branches'), \n",
    "                              dtype='float32', **voxelize_branch_parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics\n",
    "\n",
    "Densities or intensity measures of each sample in considered regions or annotated\n",
    "brain areas between different groups of samples can be compared using the independent\n",
    "two sample student t-test assuming unequal variances. \n",
    "\n",
    "ClearMap as a discovery tool also provides correction for p-values for multiple \n",
    "comparison to q-values to control for false-discovery rate.\n",
    "\n",
    "See the :mod:`ClearMap.Analysis.Statistics` module for more details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
